<!doctype html>
<html lang="en">

  <head>
    <meta charset="utf-8">

    <title>Node.js: Del I</title>

    <meta name="description" content="Foredrag om bruk av streams i javascript">
    <meta name="author" content="Mikael Brevik">

    <meta name="apple-mobile-web-app-capable" content="yes" />
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

    <link rel="stylesheet" href="css/reveal.min.css">
    <link rel="stylesheet" href="css/theme/bekk.css" id="theme">
    <link rel="stylesheet" href="css/presentation.css" id="theme">
    <link rel="stylesheet" href="css/lamp.css" id="theme">

    <!-- For syntax highlighting -->
    <link rel="stylesheet" href="lib/css/railscast.css">

    <!-- If the query includes 'print-pdf', use the PDF print sheet -->
    <script>
      document.write( '<link rel="stylesheet" href="css/print/' + ( window.location.search.match( /print-pdf/gi ) ? 'pdf' : 'paper' ) + '.css" type="text/css" media="print">' );
    </script>

    <!--[if lt IE 9]>
    <script src="lib/js/html5shiv.js"></script>
    <![endif]-->

  </head>

  <body>

    <div class="reveal">
      <div class="slides">

        <section>
          <h1>Web Audio API</h1>
          <h2>Full kontroll på lyden</h2>

          <aside class="notes">
            <p>
              Mitt navn er Mikael Brevik og jeg er fagleder for frontend her i Bekk Trondheim.
            </p>
            <p>
              I dag skal jeg prate kjapt om Web Audio API. Hva er det? Hvorfor trenger vi det og veldig grovt,
              hvordan kan vi bruke det?
            </p>
            <p>
              Og helt til slutt viser jeg et enkelt eksempel på hvordan vi kan bruke Web Audio API til å lage en "klapp"-trigger som lysbryter.
            </p>
          </aside>
        </section>

        <section>
<pre><code data-trim class="html">
&lt;bgsound src="missionimpossible.midi" loop="infinite" /&gt;
</code></pre>
          <aside class="notes">
            <p>
              Tilbake til den gode gamle IT-middelalderen, var dette den måten man kunne få en lyd på nettsiden sin.
            </p>
            <p>
              Det var ingen kontroll på når man skulle spille, hvor lenge, i hvilket volum eller hvor i sangen man skulle starte.
            </p>
            <p>
              Det var heller ikke veldig utbredt støtte for det. Det var ingen standard i en spec, men det var Microsoft som hadde
              implementert det til Internet Explorer.
            </p>
            <p>
              Når sant skal sies, kom det etterhvert noen flere alternativer, som ActiveX fra Microsoft eller gode gamle Flash.
            </p>
            <p>
              Men det var fremdeles ingen god kontroll, eller god støtte. Og kanskje det var bra på den tiden og webutviklere ikke
              var moden nok til å takle kraften av lyd.
            </p>
          </aside>
        </section>

        <section data-background="./img/geocities1.png">
            <aside class="notes">
              <p>
                Om <code>bgsound</code> var i IT-middelalderen, kan man kanskje si at Geocities var pesten. Er ikke sikkert
                det er mange som husker Geocities, selv om det ikke er mer enn 6 år siden de slutta. Men dens peak var på
                begynnelsen av 2000 tallet. Det var et hosting-selskap der man kunne legge ut egne sider - ukritisk av kvalitet.
              </p>
              <p>
                På den tiden, og særlig da Geocities-sider, var det populært å bruke <code>bgsound</code>. Det fungerte da
                selvfølgelig slik at når man gikk inn på siden så begynte det å spille musikk. Det var vanskelig å hindre
                og det var ingen indikasjon på hvilken side som hadde denne musikken.
              </p>
              <p>
                Ettersom båndbredden ikke var det beste og heller ikke støtten i nettleseren, var det ikke fin MP3 musikk som
                ble spilt, men midi-filer.
              </p>
            </aside>
        </section>

        <section data-background="./img/geocities2.png">
          <aside class="notes">
            <p>
              Her er noen ekstra eksempler på fine Geocities sider.
            </p>
          </aside>
        </section>

        <section data-background="./img/geocities3.png">
        </section>

        <section data-background="./img/giforama.gif">
        </section>

        <section data-background="#0a3c59">
          <h1>Behov</h1>
          <h2>Trenger vi det?</h2>

          <aside class="notes">
            <p>
              Etter ganske mye tortur, ble det vel klart at dersom man skulle ha lyd i nettleseren, så trengte man mer kontroll.
            </p>
            <p>
              Men er det slik at man trenger det? Hvorfor skal man ha så god kontroll på lyd i en nettleser?
            </p>

            <p>
              Den åpenbare grunnen er nok spill. Dersom man implementerer spill i nettleseren er lyd et veldig viktig aspekt av
              opplevelsen. Man må kunne ha nøyaktig timing på når en lyd skal starte og helst hvor den kommer fra i surround, og
              kunne kombinere flere lyder.
            </p>

            <p>
              Et annet åpenbart bruksområde for bedre kontroll på lyd vil jo være lyd-verktøy for avspilling og analyse.
            </p>

            <p>
              Men det er ikke bare slike innlysende applikasjoner man kan bruke lyd til i nettleseren. Man kan bruke det til
              å lage mye rikere nettsider, der lyd kan være en dimensjon av innholdet eller designet. Men man kan også bruke det
              for å hjelpe brukere med særlige behov. F.eks kan man ha språkstøtte på siden, eller bruke lyd for å finne frem på
              siden.
            </p>
          </aside>
        </section>

        <section data-background="./img/JavaScript-logo.png"  data-background-size="400px" data-background-repeat="repeat">

          <aside class="notes">
            <p>
              Så, hvordan kan man oppnå full kontroll over lyd i nettleseren? Javascript! Mozilla har jobbet med et API som heter
              Web Audio API som tillater kontroll av både timing og analysering av lyd!
            </p>
          </aside>

        </section>


        <section data-background="#0a3c59">
          <h1>Hvordan?</h1>
          <h2>Starter med <code>AudioContext</code></h2>

<pre><code data-trim class="javascript">
var audioCtx = new (window.AudioContext || window.webkitAudioContext)();
</code></pre>

          <aside class="notes">
            <p>
              Du vil som regel kun ha en audio context i dokumentet ditt. Det er mulig med flere,
              slik som du kan med f.eks Canvas Context, men som regel vil du at lyden din vil leve
              i samme context. Slik kan man f.eks si at en lyd skal være påvirket av en annen, eller velge
              at de kommer fra forskjellige retninger.
            </p>
            <p>
              Ut i fra konteksten har man mange metoder man kan bruke, og det er et forholdsvis stort API, men
              greit å skjønne når man ser det visualisert.
            </p>
          </aside>
        </section>

        <section data-background="#0a3c59">
          <h1>Routing Graph</h1>

          <img src="./img/audio_graph.png" alt="Audio Graph" width="600px" />

          <aside class="notes">
            <p>Det finnes 4 forskjellige typer noder man kan lage:</p>

            <p>Source nodes: Sound sources such as audio buffers, live audio inputs, audio-tags, oscillators, and JS processors.</p>
            <p>Modification nodes: Filters, convolvers, panners, JS processors, etc.</p>
            <p>Analysis nodes: Analyzers and JS processors</p>
            <p>Destination nodes: Audio outputs and offline processing buffers</p>

            <p>I tillegg til disse AudioNode-ne, har man noe som heter AudioParam, som er parameter for nodene. En node kan f.eks ha et parameter
            for å stille på frekvensen, eller volumet.</p>
          </aside>
        </section>

        <section data-background="#0a3c59">
<pre><code data-trim class="javascript">
var source = audioCtx.createMediaStreamSource(stream);
source.connect(analyser);
analyser.connect(distortion);
distortion.connect(biquadFilter);
biquadFilter.connect(convolver);
convolver.connect(gainNode);
gainNode.connect(audioCtx.destination);
</code></pre>

          <aside class="notes">
            <p>
              Her ser man hvordan man kan koble sammen forskjellige noder for å representere en graf.
            </p>
          </aside>
        </section>

        <section data-background="#0a3c59">
          <img src="./img/audio_graph2.png" alt="Audio Graph" height="750px" />

          <aside class="notes">
            <p>
              Koden fra forrige slide vil lage en graf som dette.
            </p>

          </aside>
        </section>

        <section>
<pre><code data-trim class="javascript">
// Get and decode asynchronously
var request = new XMLHttpRequest();
request.open('GET', url, true);
request.responseType = 'arraybuffer';
request.onload = () =>
  context.decodeAudioData(request.response, playSound);
request.send();

function playSound(buffer) {
  var source = context.createBufferSource();
  source.buffer = buffer;
  source.connect(context.destination);
  source.start(0);
}
</code></pre>

          <aside class="notes">
            <p>
              Her ser vi et eksempel på hvordan vi kan bruke en SourceNode til å spille av musikk som vi henter via et AJAX-kall.
            </p>
            <p>
              Man må hente ut sangen som et buffer, altså liste over readonly binærdata, så må man dekode buffer-data for å så lage
              en source node av det med gitt kontekst. Her ser vi også <code>source.start(0)</code> som sier start etter 0 sekunder.
            </p>
            <p>
              Start er mer nøyaktig enn det f.eks setTimeout er. setTimeout lover deg bare at noe skal skje minimum etter X millisekunder,
              den kommer ikke med en garanti om at etter X millisekunder så skjer det. En viktig del for timing av lyd med andre ord.
            </p>
            <p>
              Vi ser også hvordan vi kan koble til høytalerene i dette tilfellet.
            </p>
          </aside>
        </section>

        <section>
<pre><code data-trim class="javascript">
var analyser = context.createAnalyser();
analyser.smoothingTimeConstant = SMOOTHING;
analyser.fftSize = FFT_SIZE;

var freqs = new Uint8Array(this.analyser.frequencyBinCount);
analyser.getByteFrequencyData(freqs);
</code></pre>

          <aside class="notes">
            <p>
              Her er et eksempel på en analyse-node. Spesifikt er dette for å finne frekvensdomenet fra inputten med FFT (fast fourier transformation).
              Dette vil være en oversikt over frekvenser der x-aksen er frekvenser og y-aksen er insensiteten av frekvensen. Dette er noe
              som ofte blir brukt for å analysere lyd. Fordelen er at man kan skille mellom forskjellige lyder og f.eks fjerne white noise.
            </p>

            <p>
              fftSize er antall frekvenser vi ønsker å hente ut (i 2-erpotens), og smoothing er tidskonstanten for hvor mye man skal "fade ut" frekvenser
              mellom utbytte. Dersom den er 0 vil det være veldig harde bytter mellom hver iterasjon eller hver tick av tid.
            </p>
          </aside>
        </section>

        <section>
<pre><code data-trim class="javascript">
navigator.getUserMedia({ audio: true }, function (stream) {
  var source = context.createMediaStreamSource(stream);
  source.connect(self.analyser);
  requestAnimFrame(draw);
}, console.error.bind(console));
</code></pre>

          <aside class="notes">
            <p>
              Her er et annet eksempel på en kilde-node. Om man bruker WebRTC og getUserMedia så kan man få tak i en media stream
              som man kan da bruke direkte. Vi kan koble den via en analyse-node, hvor vi kan transformere den til FFT. Ved å bruke
              Canvas-et kan vi tegne ut frekvensene og se hvilken data vi får inn via mikrofonen.
            </p>
          </aside>
        </section>

        <section>
          <h1>Visualisering</h1>
          <p><button class="run-example-1">Kjør!</button></p>
          <canvas class="example-1-canvas"></canvas>

          <aside class="notes">
            <p>
              Her ser vi hvordan de to eksemplene fra tidligere kan flettes sammen og visualisere frekvensdomenet til lydene
              som kommer fra mikrofonen.
            </p>

            <p>
              Så om vi vil lage en bryter for å slå på og av et lys, kan vi bruke frekvensene til å prøve å kjenne igjen et
              klapp. Om dere ser nå når jeg klapper, så ser vi at flesteparten av frekvensene blir høye i intensitet, i motsetning
              til når jeg bare snakker da enkelte av frekvensene vil være høy i intensivitet.
            </p>

            <p>
              Så, om vi bruker den informasjonen, kan vi lage en slags zero-crossing-variasjon for å detektere om det er klapp
              eller ikke.
            </p>
          </aside>
        </section>

        <section>
<pre><code data-trim class="javascript">
var crossings = filter(frequencyData, function (amp) {
  return amp >= threshold;
}).length;

var isClap = crossings > 20;
if (isClap) numberCrossingsInRow++;
if (!isClap &amp;&amp; numberCrossingsInRow > 0 &amp;&amp; numberCrossingsInRow &lt; 4) {
  numberCrossingsInRow = 0;
  doSomethingOnClap();
}
</code></pre>

          <aside class="notes">
            <p>
            </p>
          </aside>
        </section>

        <section>
          <button class="run-example-2">Kjør!</button>
          <div id="lamp">
            <div class="lamp">
              <div class="gonna-give-light"></div>
            </div>
          </div>

          <aside class="notes">
          </aside>
        </section>

        <section class="center">
          <ul>
            <li><a href="http://studio.substack.net/clown">http://studio.substack.net/clown</a></li>
            <li><a href="http://aikelab.net/websynth/">http://aikelab.net/websynth/</a></li>
            <li><a href="http://webaudio.prototyping.bbc.co.uk/ring-modulator/">http://webaudio.prototyping.bbc.co.uk/ring-modulator/</a></li>
          </ul>

          <aside class="notes">
          </aside>
        </section>

      </div>
    </div>

    <script src="lib/js/head.min.js"></script>
    <script src="js/reveal.min.js"></script>
    <script src="js/common.js"></script>
    <script src="js/visualizer.js"></script>
    <script src="js/clapper.js"></script>

    <script>

      // Full list of configuration options available here:
      // https://github.com/hakimel/reveal.js#configuration
      Reveal.initialize({
        controls: false,
        progress: false,
        history: true,
        margin: 0.05,

        width: '100%',
        height: '100%',

        center: false,
        transition: 'linear',
        backgroundTransition: 'slide',

        // Optional libraries used to extend on reveal.js
        dependencies: [
          { src: 'lib/js/classList.js', condition: function() { return !document.body.classList; } },
          { src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } },
          { src: 'plugin/zoom-js/zoom.js', async: true, condition: function() { return !!document.body.classList; } },
          { src: 'plugin/notes/notes.js', async: true, condition: function() { return !!document.body.classList; } }
        ]
      });

    </script>

  </body>
</html>
